---
title: "Part 4: Tidymodels Workflowsets"
favicon: ./IMAGES/ERA_logo_circle.png
description: |
  On this Part 4 page we will analyse the same agroforestry data from ERA, as in Part 3, but we are going to make use of the relatively new tidymodels package called "Workflowsets". We are going to skip the preliminary explorertive data analysis (EDA) and jump straight to the modelling setup.  "Show code" to view the R codes used to perform a given analysis or visualise a analysis output. As you have seen in Part 3, tidymodels builds on the integration of model specifications from the parsnip package and data pre-processing steps using the recipe package. These are integrated into a workflow using the workflows package, and finally tuned using the tune package. What is revolutionary about the latest member in the tidymodels family is the workflowsets package that can create a workflow set that holds multiple workflow objects (integrated model specifications and pre-processing steps). This object, or set, can then easily be tuned or fitted based on resamples by using a set of simple commands. Workflowsets is a powerful concept for modelling as researchers often are interested in test and compare the performance of multiple models specified under different pre-processing settings. 
bibliography: library.bib
csl: frontiers-in-ecology-and-the-environment.csl
output: 
  distill::distill_article:
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: true
    code_folding: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Here we are going to explore Machine Learning with Tidymodels on the ERA agroforestry data

These objects can be created by crossing all combinations of pre-processors (e.g., formula, recipe, etc) and model specifications. This different model toolkit in the arsenal of 

# Loading necessary R packages and ERA data

**Loading general R packages**

This part of the document is where we actually get to the nitty-gritty of the ERA agroforestry data and therefore it requires us to load a number of R packages for both general Explorortive Data Analysis and Machine Learning. 

```{r Loading packages needed, comment=NA, code_folding=TRUE}
# Using the pacman functions to load required packages

        if(!require("pacman", character.only = TRUE)){
          install.packages("pacman",dependencies = T)
          }

# ------------------------------------------------------------------------------------------
# General packages
# ------------------------------------------------------------------------------------------
required.packages <- c("tidyverse", "tidymodels", "finetune", "kernlab", "here", "hablar", "spatialsample", 
                       "stacks", "rules", "baguette", "viridis", "yardstick", "DALEXtra", 
# ------------------------------------------------------------------------------------------
# Parallel computing packages
# ------------------------------------------------------------------------------------------
                      "parallelMap", "parallelly", "parallel", "doParallel"
)

p_load(char=required.packages, install = T,character.only = T)
```

## STEP 1: Getting the data

```{r Getting the data, eval=TRUE, code_folding=FALSE}
agrofor.biophys.modelling.data.wf <- readRDS(file = here::here("agrofor.biophys.modelling.data.RDS"))

ml.data.wf <-  agrofor.biophys.modelling.data.wf %>%
  dplyr::select(-c("RR", "ID", "AEZ16s", "Country", "MeanC", "MeanT", "PrName.Code", "SubPrName"))
```

Removing outliers from logRR

```{r Outliers in outcome logRR workflowsets, eval=TRUE, code_folding=FALSE}
is_outlier <- function(x) {
  return(x < quantile(x, 0.25) - 3 * IQR(x) | x > quantile(x, 0.75) + 3 * IQR(x))
}

ml.data.outliers.wf <- ml.data.wf %>%
  rationalize(logRR) %>%
  drop_na(logRR) %>%
  mutate(ERA_Agroforestry = 1) %>%
  group_by(ERA_Agroforestry) %>%
  mutate(logRR.outlier = ifelse(is_outlier(logRR), logRR, as.numeric(9999))) %>%
  ungroup()
```



