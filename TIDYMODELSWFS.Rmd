---
title: "Part 4: Tidymodels Workflowsets"
favicon: ./IMAGES/ERA_logo_circle.png
description: |
  On this Part 4 page we will analyse the same agroforestry data from ERA, as in Part 3, but we are going to make use of the relatively new tidymodels package called "Workflowsets". We are going to skip the preliminary explorertive data analysis (EDA) and jump straight to the modelling setup.  "Show code" to view the R codes used to perform a given analysis or visualise a analysis output. As you have seen in Part 3, tidymodels builds on the integration of model specifications from the parsnip package and data pre-processing steps using the recipe package. These are integrated into a workflow using the workflows package, and finally tuned using the tune package. What is revolutionary about the latest member in the tidymodels family is the workflowsets package that can create a workflow set that holds multiple workflow objects (integrated model specifications and pre-processing steps). This object, or set, can then easily be tuned or fitted based on resamples by using a set of simple commands. Workflowsets is a powerful concept for modelling as researchers often are interested in test and compare the performance of multiple models specified under different pre-processing settings. 
bibliography: library.bib
csl: frontiers-in-ecology-and-the-environment.csl
output: 
  distill::distill_article:
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: true
    code_folding: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Here we are going to explore Machine Learning with Tidymodels on the ERA agroforestry data

These objects can be created by crossing all combinations of pre-processors (e.g., formula, recipe, etc) and model specifications. This different model toolkit in the arsenal of 

# Loading necessary R packages and ERA data

**Loading general R packages**

This part of the document is where we actually get to the nitty-gritty of the ERA agroforestry data and therefore it requires us to load a number of R packages for both general Explorortive Data Analysis and Machine Learning. 

```{r Loading packages needed, comment=NA, code_folding=TRUE}
# Using the pacman functions to load required packages

        if(!require("pacman", character.only = TRUE)){
          install.packages("pacman",dependencies = T)
          }

# ------------------------------------------------------------------------------------------
# General packages
# ------------------------------------------------------------------------------------------
required.packages <- c("tidyverse", "tidymodels", "finetune", "kernlab", "here", "hablar", "spatialsample", 
                       "stacks", "rules", "baguette", "viridis", "yardstick", "DALEXtra", 
# ------------------------------------------------------------------------------------------
# Parallel computing packages
# ------------------------------------------------------------------------------------------
                      "parallelMap", "parallelly", "parallel", "doParallel"
)

p_load(char=required.packages, install = T,character.only = T)
```

## STEP 1: Getting the data

```{r Getting the data, eval=TRUE, code_folding=FALSE}
agrofor.biophys.modelling.data.wf <- readRDS(file = here::here("agrofor.biophys.modelling.data.RDS"))

ml.data.wf <-  agrofor.biophys.modelling.data.wf %>%
  dplyr::select(-c("RR", "ID", "AEZ16s", "Country", "MeanC", "MeanT", "PrName.Code", "SubPrName"))
```

Removing outliers from logRR

```{r Outliers in outcome logRR workflowsets, eval=TRUE, code_folding=FALSE}
is_outlier <- function(x) {
  return(x < quantile(x, 0.25) - 3 * IQR(x) | x > quantile(x, 0.75) + 3 * IQR(x))
}

ml.data.outliers.wf <- ml.data.wf %>%
  rationalize(logRR) %>%
  drop_na(logRR) %>%
  mutate(ERA_Agroforestry = 1) %>%
  group_by(ERA_Agroforestry) %>%
  mutate(logRR.outlier = ifelse(is_outlier(logRR), logRR, as.numeric(9999))) %>%
  ungroup()
```


Agroforestry data with no outliers

```{r Agroforestry data withoout outliers workflowsets, code_folding=FALSE}
ml.data.no.outliers.wf <-  ml.data.outliers.wf %>%
  dplyr::filter(logRR.outlier == 9999) %>%
  dplyr::select(-c(ERA_Agroforestry, logRR.outlier))
```


Notice we do not remove missing values

## STEP 2: Splitting data

Split data in training and testing sets

```{r data splitting workflowsets, code_folding=FALSE}
set.seed(456)

# Splitting data
af.split.wf <- initial_split(ml.data.no.outliers.wf, prop = 0.80, strata = logRR)

af.train.wf <- training(af.split.wf)
af.test.wf <- testing(af.split.wf)
```

## STEP 3: Define resampling techniques on training data

Notice here that we do not have any repeats defined as we did originally. Previously we used an argument: repeats = 10 to specify the number of times to repeat the V-fold partitioning. Instead we are increasing the number of partitions/folds (v) from 10 to 20, for the cv-folds, just as the spatial clustering cv-folds.

```{r Resampling techniques workflowsets, code_folding=FALSE, eval=TRUE}
set.seed(345)

# Re-sample technique(s) 
boostrap.wf <- bootstraps(af.train.wf, times = 20, strata = logRR)
cv.fold.wf <- vfold_cv(af.train.wf, v = 10, repeats = 5)
spatial.cv.fold.wf <- spatial_clustering_cv(af.train.wf, coords = c("Longitude", "Latitude"), v = 10)
```

## STEP 4: Define model metrics

```{r Model metrics for workflowsets, code_folding=FALSE, eval=TRUE}
# Metrics
multi.metric.wf <- metric_set(yardstick::rmse, yardstick::rsq, yardstick::ccc, yardstick::mae)

model.control.wf <- control_stack_grid() # save_pred = TRUE, save_workflow = TRUE.
model.control.linear.wf <- control_resamples(save_pred = TRUE)
```


## STEP 5: Create pre-processing recipies

```{r code_folding=FALSE, eval=TRUE}
base_recipe <- 
  recipe(formula = logRR ~ ., data = af.train.wf) %>%
  update_role(Site.Type, new_role = "predictor") %>% # alters an existing role in the recipe to variables.
  update_role(PrName, # or assigns an initial role to variables that do not yet have a declared role.
              Out.SubInd,
              Out.SubInd.Code,
              Product,
              Latitude,
              Longitude,
              Tree,
              new_role = "sample ID") 
# ------------------------------------------------------------------------------------------------------------------------------------------------
   

impute_mean_recipe <- 
  base_recipe %>%
  step_impute_mean(all_numeric_predictors(), skip = FALSE) %>%
  step_novel(Site.Type, skip = FALSE) %>% 
  step_dummy(Site.Type, one_hot = TRUE, naming = partial(dummy_names,sep = "_"), skip = FALSE) %>%
  step_zv(all_predictors(), skip = FALSE) %>% # remove any columns with a single unique value
  step_nzv(all_predictors(), skip = FALSE)
  

impute_knn_recipe <- 
  base_recipe %>%
  step_impute_knn(all_numeric_predictors(), skip = FALSE) %>%
  step_novel(Site.Type, skip = FALSE) %>% 
  step_dummy(Site.Type, one_hot = TRUE, naming = partial(dummy_names,sep = "_"), skip = FALSE) %>%
  step_zv(all_predictors(), skip = FALSE) %>% # remove any columns with a single unique value
  step_nzv(all_predictors(), skip = FALSE) 
   

normalize_recipe <- 
  base_recipe %>%
  step_impute_linear(all_numeric_predictors(), impute_with = imp_vars(Longitude, Latitude), skip = FALSE) %>% # create linear regression models to impute missing data.
  step_novel(Site.Type, skip = FALSE) %>% 
  step_dummy(Site.Type, one_hot = TRUE, naming = partial(dummy_names,sep = "_"), skip = FALSE) %>%
  step_zv(all_predictors(), skip = FALSE) %>% # remove any columns with a single unique value
  step_nzv(all_predictors(), skip = FALSE) %>%
  step_normalize(all_numeric_predictors(), skip = FALSE) # normalize numeric data: standard deviation of one and a mean of zero.

rm_corr_recipe <- 
  base_recipe %>% 
  step_impute_linear(all_numeric_predictors(), impute_with = imp_vars(Longitude, Latitude), skip = FALSE) %>% # create linear regression models to impute missing data.
  step_novel(Site.Type, skip = FALSE) %>% 
  step_dummy(Site.Type, one_hot = TRUE, naming = partial(dummy_names,sep = "_"), skip = FALSE) %>%
  step_zv(all_predictors(), skip = FALSE) %>% # remove any columns with a single unique value
  step_nzv(all_predictors(), skip = FALSE) %>%
  step_corr(all_numeric_predictors(), threshold = 0.8, method = "pearson", skip = FALSE)

interact_recipe <- 
  base_recipe %>% 
  step_impute_linear(all_numeric_predictors(), impute_with = imp_vars(Longitude, Latitude), skip = FALSE) %>% # create linear regression models to impute missing data.
  step_novel(Site.Type, skip = FALSE) %>% 
  step_dummy(Site.Type, one_hot = TRUE, naming = partial(dummy_names,sep = "_"), skip = FALSE) %>%
  step_zv(all_predictors(), skip = FALSE) %>% # remove any columns with a single unique value
  step_nzv(all_predictors(), skip = FALSE) %>%
  step_interact(~ all_numeric_predictors():all_numeric_predictors(), skip = FALSE)
```

**Noet:** To view how the recipe pre-process the data simply pipe it into a prep() function to prepare it, then a juice() function to extract it and then its a good idea to make use of the glimpse() function to easily see each variable. 

```{r  code_folding=FALSE, eval=TRUE}
impute_knn_recipe %>% prep() %>% juice() %>% glimpse()
```

A recipe object is of the class "recipe" and can also be viewed using the summary() function. This will show the different feature types:

```{r code_folding=FALSE, eval=TRUE}
summary(impute_knn_recipe) 
```

## STEP 6: Defining model specifications

```{r code_folding=FALSE, eval=FALSE}
lm_spec <- linear_reg() %>% 
  set_mode("regression") %>% 
  set_engine("lm") 

glm_spec <- linear_reg(
  penalty = tune(),
  mixture = tune()) %>%
  set_engine("glmnet") %>% 
  set_mode("regression")

cart_spec <- 
   decision_tree(cost_complexity = tune(), 
                 min_n = tune()) %>% 
   set_engine("rpart") %>% 
   set_mode("regression")

mars_spec <- 
   mars(prod_degree = tune()) %>%  #<- use GCV to choose terms
   set_engine("earth") %>% 
   set_mode("regression")

knn_spec <- 
   nearest_neighbor(neighbors = tune(), 
                    weight_func = tune()) %>% 
   set_engine("kknn") %>% 
   set_mode("regression")

svm_p_spec <- 
   svm_poly(cost = tune(), 
            degree = tune()) %>% 
   set_engine("kernlab") %>% 
   set_mode("regression")

cubist_spec <- 
   cubist_rules(committees = tune(), 
                neighbors = tune()) %>% 
   set_engine("Cubist") 

nnet_spec <- 
   mlp(hidden_units = tune(), 
       penalty = tune(), 
       epochs = tune()) %>% 
   set_engine("nnet", MaxNWts = 2600) %>% 
   set_mode("regression")

rf_spec <- 
   rand_forest(mtry = tune(), 
               min_n = tune(), 
               trees = tune()) %>% 
   set_engine("ranger") %>% 
   set_mode("regression")

xgb_spec <- 
   boost_tree(tree_depth = tune(), 
              learn_rate = tune(), 
              loss_reduction = tune(), 
              min_n = tune(), 
              sample_size = tune(), 
              trees = tune()) %>% 
   set_engine("xgboost") %>% 
   set_mode("regression")
```


## STEP 7: Integrating pre-processing and model specification into a workflowset

```{r code_folding=FALSE, eval=FALSE}
wflwset_setup <- 
   workflow_set(
      preproc = list(impute_mean = impute_mean_recipe,
                     impute_knn = impute_knn_recipe,
                     normalized = normalize_recipe,
                     rm_corr = rm_corr_recipe,
                     interaction = interact_recipe),
      models = list(lm = lm_spec,
                    glm = glm_spec, 
                    cart = cart_spec, 
                    mars = mars_spec,
                    knn = knn_spec,
                    svm_p = svm_p_spec,
                    cubist = cubist_spec,
                    nnet = nnet_spec,
                    RF = rf_spec,
                    XGB = xgb_spec),
      cross = TRUE
   )

saveRDS( wflwset_setup, file = here::here("TidyModWflSet_OUTPUT", "wflwset_setup.RDS")) 
```

```{r code_folding=FALSE, eval=FALSE}
wflwset_setup <- readRDS(here::here("TidyModWflSet_OUTPUT", "wflwset_setup.RDS"))
wflwset_setup
```

We see very clearly that a workflowset contains all possible combinations of model specifications and pre-processing steps. A powerful way to test several models on different forms of pre-processing specifications to scan and find the best setup. 

## STEP 8: Tuning workflowsets using tune_grid() or tune_race_anova()

There are different ways we can tune/fit our workflowsets. The more traditional one is using tune_grid() from the tune package, that is part of tidymodels. With tune_grid() we will compute a set of performance metrics (e.g. MAE or RMSE) for a pre-defined set of tuning parameters that correspond to a model or recipe across one or more resamples of the data. Hence all set combinations of (hyper)-parameters for the models are used to tune the models individually. This can sometimes cause really long and expensive model tuning. 

A more effecient alternative to tune_grid() is the tune_race_anova() from the finetune package. Instead of tuning the individual models based on all possible combinations of (hyper)-parameters tune_race_anova() computes a set of performance metrics (e.g. MAE or RMSE) for a pre-defined set of tuning parameters that correspond to a model or recipe across one or more resamples of the data. After an initial number of resamples have been evaluated, the process eliminates tuning parameter combinations that are unlikely to be the best results using a repeated measure ANOVA model. This approach is significantly more efficient, as not all (hyper)-parameter combination will have to be used in the model tuning process, but only the ones that in a step-wise manner improve model performance. 

### Using tune_grid()

```{r code_folding=FALSE, eval=FALSE}
 # Initializing parallel processing 
 doParallel::registerDoParallel()

set.seed(123)

wflwset_setup_res <- 
   wflwset_setup %>% 
   # The first argument is a function name from the {{tune}} package such as `tune_grid()`, `fit_resamples()`, etc.
   workflow_map(fn        = "tune_grid", 
                resamples = cv.fold.wf, 
                grid      = 5, 
                metrics   = multi.metric.wf, 
                verbose   = TRUE)

# Terminating parallel session
parallelStop()
```




### Using tune_race_anova()

```{r}
race_ctrl <-
   control_race(
      save_pred = TRUE,
      parallel_over = "everything",
      save_workflow = TRUE
   )

 # Initializing parallel processing 
 doParallel::registerDoParallel()

wflwset_setup_race_res <-
   wflwset_setup %>%
   workflow_map(fn        = "tune_race_anova", 
                seed      = 1503,
                resamples = cv.fold.wf, 
                metrics   = multi.metric.wf, 
                verbose   = TRUE, 
                grid      = 5,
                control   = race_ctrl
   )

# Terminating parallel session
parallelStop()

```


## STEP 9: Assessing results of the tuned workflowsets

### From tune_grid()

```{r}
autoplot(wflwset_setup_res)
```


```{r}
rank_results(wflwset_setup_res, 
             rank_metric = "rmse", 
             select_best = TRUE) %>% 
  select(rank, mean, model, wflow_id, .config)
```

### From tune_race_anova()

```{r}
wflwset_setup_race_res
```





























